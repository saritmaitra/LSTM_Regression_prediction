# -*- coding: utf-8 -*-
"""LSTM & crypto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EweZvgslxBoPkkuAUEfZqceyfrqL3Vm7
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
import joblib
from pandas import DataFrame, concat
pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 500)
# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.5)
rcParams['figure.figsize'] = 16, 10

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
#tf.random.set_seed(RANDOM_SEED)

from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()
!pip install cryptocompare
import cryptocompare
import requests
!pip install plotly
!pip install cufflinks
import plotly.express as px
import plotly.graph_objects as go
!pip install ta
import ta

from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Activation
from keras.optimizers import SGD
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard
import joblib
from google.colab import files
!pip install ta
import ta

from google.colab import files

# linear regression feature importance
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.feature_selection import RFECV

from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
from math import sqrt
from sklearn.metrics import r2_score
# Feature Scaling Normalization
from sklearn.preprocessing import MinMaxScaler

"""# 1 Data Ingestion:"""

url = "https://min-api.cryptocompare.com/data/histohour?fsym=BTC&tsym=USD&limit=2000"
f = requests.get(url)
ipdata = f.json()
btc = pd.DataFrame(ipdata['Data'])
btc['time'] = pd.to_datetime(btc['time'],unit='s')
btc.set_index('time',inplace=True)
print('BTC hourly price in USD:')
print(btc.tail(2)); 

print(btc.shape)

print(btc.shape)

"""## 1.1 Visualization:"""

fig = go.Figure()
fig.add_trace(go.Scatter(x = btc.index, y = btc.close,
                         marker = dict(color ="purple"), name = "Actual close price"))

fig.update_xaxes(showline = True, linewidth = 2, linecolor = 'black', mirror = True, showspikes = True,)
fig.update_yaxes(showline = True, linewidth = 2, linecolor = 'black', mirror = True, showspikes = True,)
fig.update_layout(
    title = "BTC close Price (Hourly frequency)", 
    yaxis_title = 'BTC (US$)') 
fig.update_layout(autosize = False, width = 1000, height = 400,)
fig.show()

plt.figure(figsize = (15,5))
plt.plot(btc.close)
plt.title('BTC Close price (Hourly frequency)')
plt.xlabel ('Date_time')
plt.ylabel ('Price (US$')
plt.show()

plt.figure(figsize = (10,5))
sns.distplot(btc.close)
plt.show()

"""We have somewhat skewed distribution with some values in the 12,000 range (we might want to explore those). We’ll use a trick - log transformation:"""

plt.figure(figsize = (10,5))
sns.distplot(np.log1p(btc.close).diff())
plt.show()

"""## 1.2 lognormal distribution: 
log of the data becomes normalish and we can take advantage of many features of a normal distribution, like well-defined mean, standard deviation (and hence z-scores), symmetry, etc.

"""

from scipy.stats import lognorm
from scipy import stats

fig, ax = plt.subplots(figsize=(10, 6))

values = np.log1p(btc.close).diff().dropna()

shape, loc, scale = stats.lognorm.fit(values) 
x = np.linspace(values.min(), values.max(), len(values))
pdf = stats.lognorm.pdf(x, shape, loc=loc, scale=scale) 
label = 'mean=%.4f, std=%.4f, shape=%.4f' % (loc, scale, shape)

ax.hist(values, bins=30)
ax.plot(x, pdf, 'r-', lw=2, label=label)
ax.legend(loc='best')
plt.show()

"""Using lognormal distribution. 
log of the data becomes normalish and we can take advantage of many features of a normal distribution, like well-defined mean, standard deviation (and hence z-scores), symmetry, etc.

"""

# Feature Scaling Normalization
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler() # min-max normalization and scale the features in the 0-1 range.
close_price = btc['close'].values.reshape(-1, 1) # The scaler expects the data to be shaped as (x, y)
scaled_close = scaler.fit_transform(close_price)

scaled_close = scaled_close[~np.isnan(scaled_close)] # removing NaNs (if any)
scaled_close = scaled_close.reshape(-1, 1) # reshaping data after removing NaNs

"""## 1.3 Making sequences:
- LSTMs expect the data to be in 3 dimensions. We need to split the data into sequences of some preset length. The shape we want to obtain is: [batch_size, sequence_length, n_features]
- We also want to save some data for testing. Let’s build some sequences:


"""

SEQ_LEN = 100 # creating a sequence of 100 hours at position 0.
def to_sequences(data, seq_len):
  d = []
  
  for index in range(len(data) - seq_len):
    d.append(data[index: index + seq_len])
  return np.array(d)
  
def preprocess(data_raw, seq_len, train_split):
  data = to_sequences(data_raw, seq_len)
  num_train = int(train_split * data.shape[0])
  X_train = data[:num_train, :-1, :]
  y_train = data[:num_train, -1, :]
  
  X_test = data[num_train:, :-1, :]
  y_test = data[num_train:, -1, :]
  
  return X_train, y_train, X_test, y_test

"""
Walk forward validation:
Initial SEQ_LEN is defiend above, so, walk forward will be shifting one position to the right and create another sequence. 
The process is repeated until all possible positions are used.
"""
X_train, y_train, X_test, y_test = preprocess(scaled_close, SEQ_LEN, train_split = 0.95) # 5% of the data saved for testing. 
print(X_train.shape, X_test.shape)

"""
The number of periods in and out can be any number of our choosing.
Our model will use 1805 sequences representing 99 days of Bitcoin price changes each for training.
We’re going to predict the price for 96 hours in the future
"""

"""# 2 Building Univariate LSTM model:
We’re creating a 3 layer LSTM Recurrent Neural Network. We use Dropout with a rate of 20% to combat overfitting during training:
"""

DROPOUT = 0.2 # 20% Dropout 20% is used to control overfitting during training
WINDOW_SIZE = SEQ_LEN - 1

model = keras.Sequential()
model.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=True),
                        input_shape=(WINDOW_SIZE, X_train.shape[-1])))
"""
Bidirectional RNNs allows to train on the sequence data in forward and backward direction.
CuDNNLSTM is Fast LSTM implementation backed by cuDNN.
"""
model.add(Dropout(rate=DROPOUT))
model.add(Bidirectional(LSTM((WINDOW_SIZE * 2), return_sequences=True)))
model.add(Dropout(rate=DROPOUT))
model.add(Bidirectional(LSTM(WINDOW_SIZE, return_sequences=False)))
model.add(Dense(units=1))
model.add(Activation('linear'))
"""
Output layer has a single neuron (predicted Bitcoin price). We use Linear activation function
which activation is proportional to the input.
"""
BATCH_SIZE = 64

model.compile(loss='mean_squared_error', optimizer='adam')
history = model.fit(X_train, y_train, epochs=50, batch_size=BATCH_SIZE,
                    shuffle=False, validation_split=0.1) # shuffle not advisable during training of Time Series

history.history.keys()
# finding the keys to use for plotting

# history for loss
plt.figure(figsize = (10,5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## 2.1 Actual vs Prediction (test harness):"""

y_pred = model.predict(X_test) # prediction on test data

y_test_inverse = DataFrame(scaler.inverse_transform(y_test)) # invert the test to original values
y_test_inverse.index = btc.index[-len(y_test):] # assigning datetime
print('Test data:',)
print(y_test_inverse.tail(3)); print();

y_pred_inverse = DataFrame(scaler.inverse_transform(y_pred)) # invert the prediction to understandable values
y_pred_inverse.index = y_test_inverse.index # assigning datetime
print('Prediction data:',)
print(y_pred_inverse.tail(3))

"""## 2.2 Accuracy metrics:"""

print(f'MAE {mean_absolute_error(y_test, y_pred)}')
print(f'MSE {mean_squared_error(y_test, y_pred)}')
print(f'RMSE {np.sqrt(mean_squared_error(y_test, y_pred))}')
print(f'R2 {r2_score(y_test, y_pred)}')

"""## 2.3 Actual vs prediction plot:"""

# history for loss
plt.figure(figsize = (15,5))
plt.plot(y_test_inverse)
plt.plot(y_pred_inverse)
#plt.plot(predictions_future.FuturePrediction)
plt.title('Actual vs Prediction plot (Price prediction model)')
plt.ylabel('price')
plt.xlabel('date')
plt.legend(['actual', 'prediction'], loc='upper left')
plt.show()

"""An interesting direction of future investigation might be analyzing the correlation between different cryptocurrencies and how would that affect the performance of our model.

## 2.4 Saving model:
"""

model.save("BTCPrediction_model.h5")

# download from the notebook
files.download("BTCPrediction_model.h5")

"""# 3 Multi-Variate:

## 3.1 Feature engineering & data processing:
"""

## Technical Indicators
btc['volume'] = btc.volumefrom - btc.volumeto

# Adding all the indicators
btc = ta.add_all_ta_features(btc, open="open", high="high", low="low", 
                            close="close", volume="volume", fillna=True)

# Dropping Open', 'High', 'Low', 'Volume' 
btc.drop(['open', 'high', 'low', 'volume'], axis=1, inplace=True)
print(btc.columns);print(); print(btc.shape)

"""## 3.2 Recursive feature elimination:"""

y = btc.close
X = btc.drop(['close'], axis=1)
names = DataFrame(X.columns)

#Feature ranking with recursive feature elimination and cross-validated selection of the best number of features

#use linear regression as the model
lin_reg = LinearRegression()

#This is to select 8 variables: can be changed and checked in model for accuracy
mod =  RFECV(lin_reg, step=1, cv=20) #RFE(lin_reg, 4, step=1)

mod_fit = mod.fit(X,y) #to fit

#The feature ranking, such that ranking_[i] corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.

rankings=DataFrame(mod_fit.ranking_, index=names) #Make it into data frame
rankings.rename(columns ={0: 'Rank'}, inplace=True)
rankings.transpose()

# Selecting features to be involved intro training and predictions

columns = ['others_cr','trend_macd', 'trend_ichimoku_a', 'trend_ichimoku_conv', 'trend_ichimoku_base', 'close']
df = btc[['others_cr','trend_macd', 'trend_ichimoku_a', 'trend_ichimoku_conv', 'trend_ichimoku_base', 'close']]

train = df.astype(float)

print('Shape of training set == {}.'.format(train.shape)); print();
print(df.tail()); print();
print('Observations: %d' % (len(df)));

fig, ax = plt.subplots(figsize=(10, 6))

values = df.close

shape, loc, scale = stats.lognorm.fit(values) 
x = np.linspace(values.min(), values.max(), len(values))
pdf = stats.lognorm.pdf(x, shape, loc=loc, scale=scale) 
label = 'mean=%.4f, std=%.4f, shape=%.4f' % (loc, scale, shape)

ax.hist(values, bins=30)
ax.plot(x, pdf, 'r-', lw=2, label=label)
ax.legend(loc='best')
plt.show()

train_size = int(len(df) * 0.80)
train, test = df[0:train_size], df[train_size:len(df)]
print('Observations: %d' % (len(df)))
print('Training Observations: %d' % (len(train)))
print('Testing Observations: %d' % (len(test)))
print('Training set shape == {}'.format(train.shape))

train = train.astype(float)

print('Shape of training set == {}.'.format(train.shape))
print(train.tail())

print('Min:', np.min(train))
print('Max:', np.max(train))

"""## 3.3 Feature scaling:"""

minmax = MinMaxScaler(feature_range = (0, 1))
minmax_single = MinMaxScaler(feature_range = (0, 1))

train_sc = np.concatenate([minmax.fit_transform(train[['others_cr','trend_macd', 'trend_ichimoku_a', 
                                                       'trend_ichimoku_conv', 'trend_ichimoku_base']].values), 
                                  minmax_single.fit_transform(train[['close']].values)], axis = 1)
train_sc = DataFrame(train_sc, columns = train.columns)
train_sc

train_sc.describe()

train_sc = train_sc.values # converting to numpy array
n_lags = int(120) # look back period
n_per_out = 24 # # How many periods to predict
n_features = train_sc.shape[1]

# preparing training set
X_train = []
y_train = []
for i in range(n_lags, train_sc.shape[0]): 
  X_train.append(train_sc[i-n_lags:i])
  y_train.append(train_sc[i, 0])

X_train, y_train = np.array(X_train), np.array(y_train)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], train_sc.shape[1]))
print(X_train.shape, y_train.shape) 

# print(train_sc.shape[0], train_sc.shape[1], X_train.shape[0], X_train.shape[1])
# 1600 6 1540 60

# Check the shape (again) before start training
shape_chk = []
for i in columns:
    index = {}
    index["X_train"] = X_train.shape
    index["y_train"] = y_train.shape
    shape_chk.append(index)

pd.DataFrame(shape_chk, index=columns)

"""## 3.4 Preparing validation /Test data:"""

total_data = concat((train, test), axis = 0)
inputs = (total_data[len(total_data) - len(test) - n_lags:]).dropna()
inputs.shape

test_sc = np.concatenate([minmax.transform(inputs[['others_cr','trend_macd', 'trend_ichimoku_a', 'trend_ichimoku_conv', 
                                                   'trend_ichimoku_base']].values), 
                          minmax_single.transform(inputs[['close']].values)], axis = 1)
print(test_sc); print(); print('Shape of test data:', test_sc.shape)

DataFrame(test_sc, columns= test.columns).describe()

# shaping data from neural network
X_test = []
y_test = []
for i in range(n_lags, test_sc.shape[0]):
  X_test.append(test_sc[i-n_lags:i])
  y_test.append(test_sc[i,0])

X_test, y_test = np.array(X_test), np.array(y_test)
#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], n_features))
print(X_test)

#from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional
from keras.optimizers import SGD
import math
from sklearn.metrics import mean_squared_error
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # The LSTM architecture
# model = keras.Sequential()
# # First LSTM layer with Dropout regularisation
# model.add(LSTM(units=64, activation = 'relu', return_sequences=True, 
#                    input_shape=(X_train.shape[1], n_features)))
# model.add(Dropout(0.2))
# # Second LSTM layer
# model.add(LSTM(units=32, activation = 'relu', return_sequences=True))
# model.add(Dropout(0.1))
# # Third LSTM layer
# #model.add(LSTM(units=20, activation = 'relu', return_sequences=True))
# #model.add(Dropout(0.5))
# # Fourth LSTM layer
# #model.add(LSTM(units=20))
# #model.add(Dropout(0.5))
# # The output layer
# model.add(Dense(units=1))
# 
# # Compiling the RNN
# model.compile(optimizer=keras.optimizers.Adam(0.0001), loss = 'mean_squared_error', metrics = ['mse'])
# 
# 
# BATCH_SIZE = 32
# 
# es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)
# rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)
# mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)
# 
# tb = TensorBoard('logs')
# 
# model.compile(loss='mean_squared_error', optimizer='adam')
# history = model.fit(X_train, y_train, 
#                     epochs=20, # number of training cycles
#                     batch_size=BATCH_SIZE, 
#                     shuffle=False, # not to shuffle while training
#                     validation_split=0.1)
# 
# #history =  model.fit(x=X_train,y=y_train,
#                     #shuffle=False, # not to shuffle while training
#                     #epochs=20, # number of training cycles
#                     #validation_data= (X_test, y_test), # using some percent of the data for measuring the error and not during training
#                     #batch_size=BATCH_SIZE), # the number of training examples that are fed at a time to our model
#                     #callbacks=[es, rlr, mcp, tb],
#                      #verbose=1) # to prevent our model from overfitting when the training and validation error start to diverge

# finding the keys to use for plotting
history.history.keys()

# history for loss
plt.figure(figsize = (10,5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

DataFrame(X_test)

y_pred = model.predict(X_test)
#y_pred = np.reshape(y_pred, (y_pred.shape[0]*y_pred.shape[1], y_pred.shape[2]))

y_pred = minmax_single.inverse_transform(y_pred)
y_pred

y_pred = model.predict(X_test)
predicted_price = minmax_single.inverse_transform(y_pred)
predicted_price

from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.metrics import r2_score

print(f'MSE {mean_squared_error(y_test, y_pred)}')
print(f'RMSE {np.sqrt(mean_squared_error(y_test, y_pred))}')
print(f'R2 {r2_score(y_test, y_pred)}')